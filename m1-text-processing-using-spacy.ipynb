{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Milestone 1: Text Processing using spaCy",
   "id": "9a031c6b76937201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import spacy and the small English model",
   "id": "a90a9b8ca7901c4a"
  },
  {
   "cell_type": "code",
   "id": "173e10f57aa0d427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:54:11.038395Z",
     "start_time": "2024-10-21T23:54:10.826389Z"
    }
   },
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the data from the file ./data/data.json",
   "id": "33e3f752b1cc9391"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T23:54:11.044230Z",
     "start_time": "2024-10-21T23:54:11.041458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open('./data/data.json') as f: data = json.load(f)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a Python function that takes in a text string, performs all operations described in the previous step, and outputs a list of tokens (lemmas).",
   "id": "b75c181a7a7e694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:54:11.051003Z",
     "start_time": "2024-10-21T23:54:11.049157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(text):\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "    #Creates a spaCy document with the text lemmas and their attributes\n",
    "    doc = nlp(text)\n",
    "    #Removes stopwords, punctuation, line breaks and other unclassified lemmas.\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space and token.pos_ != 'X']\n",
    "    #Returns a list of tokens (lemmas) found in the text.\n",
    "    return tokens"
   ],
   "id": "3b59e8acd2ad12ad",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the function with the text \"The quick brown fox jumps over the lazy dog.\\nThis is a new line.\"",
   "id": "aab010561edcccb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:54:11.062586Z",
     "start_time": "2024-10-21T23:54:11.055797Z"
    }
   },
   "cell_type": "code",
   "source": "tokenize(\"The quick brown fox jumps over the lazy dog.\\nThis is a new line.\")",
   "id": "6e0b6b5135411af0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'new', 'line']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Apply the function to the 'text' field of each dictionary in the 'data' list.",
   "id": "fa02c50d323c72bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:54:12.062298Z",
     "start_time": "2024-10-21T23:54:11.069035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for d in data:\n",
    "    d['tokenized_text'] = tokenize(d['text'])"
   ],
   "id": "335bb7f44ecac33e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the modified 'data' list to a new file called './data/outputs/milestone-1/tokenized-data.json'.",
   "id": "b824b6e3f0527786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:56:47.204041Z",
     "start_time": "2024-10-21T23:56:47.196706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Create the directory if it does not exist.\n",
    "import os\n",
    "os.makedirs('./outputs/milestone-1', exist_ok=True)\n",
    "#Write the file\n",
    "with open('./outputs/milestone-1/tokenized-data.json', 'w') as f: json.dump(data, f)"
   ],
   "id": "1b6beb08522a8fb5",
   "outputs": [],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
